{
    "MeLLaMA-70B-chat": {
        "metrics": {
            "f1-weighted": [
                0.0307470672977979,
                [
                    0.013935977517860885,
                    0.05651660072587054
                ]
            ],
            "accuracy": [
                0.01592920353982301,
                [
                    0.007079646017699115,
                    0.03008849557522124
                ]
            ],
            "precision": [
                0.4407079646017699,
                [
                    0.24495691198347558,
                    0.5769911504424778
                ]
            ],
            "recall": [
                0.01592920353982301,
                [
                    0.007079646017699115,
                    0.03008849557522124
                ]
            ]
        },
        "nr_faulty_parsable": 12,
        "nr_non_parsable": 553
    },
    "Llama-2-70b-chat-hf": {
        "metrics": {
            "f1-weighted": [
                0.6233216921529403,
                [
                    0.5763315020470622,
                    0.6700013808390793
                ]
            ],
            "accuracy": [
                0.6690265486725664,
                [
                    0.6300884955752213,
                    0.7070429894698141
                ]
            ],
            "precision": [
                0.7000298802808231,
                [
                    0.6511681453703128,
                    0.7398533091811821
                ]
            ],
            "recall": [
                0.6690265486725664,
                [
                    0.6300884955752213,
                    0.7070429894698141
                ]
            ]
        },
        "nr_faulty_parsable": 0,
        "nr_non_parsable": 1
    },
    "Llama-3.1-8B-Instruct": {
        "metrics": {
            "f1-weighted": [
                0.6929492154034065,
                [
                    0.6540260357283104,
                    0.7298402630210499
                ]
            ],
            "accuracy": [
                0.6814159292035398,
                [
                    0.6424778761061947,
                    0.7185840707964601
                ]
            ],
            "precision": [
                0.7107672716964752,
                [
                    0.6712994075677101,
                    0.7466159747333423
                ]
            ],
            "recall": [
                0.6920353982300885,
                [
                    0.6530973451327433,
                    0.7292035398230089
                ]
            ]
        },
        "nr_faulty_parsable": 0,
        "nr_non_parsable": 0
    },
    "Med-LLaMA3-8B": {
        "metrics": {
            "f1-weighted": [
                0.3837980916596629,
                [
                    0.3411498562358973,
                    0.4277515628618994
                ]
            ],
            "accuracy": [
                0.36106194690265486,
                [
                    0.32212389380530976,
                    0.40176991150442476
                ]
            ],
            "precision": [
                0.6270446616507318,
                [
                    0.5600145893428012,
                    0.6813795672376389
                ]
            ],
            "recall": [
                0.44424778761061945,
                [
                    0.40530973451327434,
                    0.4849557522123894
                ]
            ]
        },
        "nr_faulty_parsable": 506,
        "nr_non_parsable": 59
    },
    "MeLLaMA-13B-chat": {
        "metrics": {
            "f1-weighted": [
                0.5633802068583779,
                [
                    0.5209049360673271,
                    0.6052422974967083
                ]
            ],
            "accuracy": [
                0.44424778761061945,
                [
                    0.40353982300884955,
                    0.4849557522123894
                ]
            ],
            "precision": [
                0.7721540274833578,
                [
                    0.7231491835388637,
                    0.8145197934116356
                ]
            ],
            "recall": [
                0.44424778761061945,
                [
                    0.40353982300884955,
                    0.4849557522123894
                ]
            ]
        },
        "nr_faulty_parsable": 76,
        "nr_non_parsable": 242
    },
    "Llama-2-13b-chat-hf": {
        "metrics": {
            "f1-weighted": [
                0.5979426893595191,
                [
                    0.5518544874465678,
                    0.6421854487134224
                ]
            ],
            "accuracy": [
                0.6106194690265486,
                [
                    0.5699115044247788,
                    0.6513274336283186
                ]
            ],
            "precision": [
                0.6447713864306784,
                [
                    0.5968552542280711,
                    0.6885707666647802
                ]
            ],
            "recall": [
                0.6106194690265486,
                [
                    0.5699115044247788,
                    0.6513274336283186
                ]
            ]
        },
        "nr_faulty_parsable": 274,
        "nr_non_parsable": 32
    },
    "Meta-Llama-3-8B-Instruct": {
        "metrics": {
            "f1-weighted": [
                0.6455390101407801,
                [
                    0.6063750812472862,
                    0.6827448818723378
                ]
            ],
            "accuracy": [
                0.6106194690265486,
                [
                    0.5699115044247788,
                    0.6495575221238938
                ]
            ],
            "precision": [
                0.6810297000677917,
                [
                    0.6399208154119856,
                    0.717725242752145
                ]
            ],
            "recall": [
                0.6442477876106195,
                [
                    0.6035398230088496,
                    0.6831858407079646
                ]
            ]
        },
        "nr_faulty_parsable": 0,
        "nr_non_parsable": 22
    },
    "gpt-4o-mini": {
        "metrics": {
            "f1-weighted": [
                0.7467454165294943,
                [
                    0.7097253539545411,
                    0.7815324178608667
                ]
            ],
            "accuracy": [
                0.7451327433628319,
                [
                    0.7079646017699115,
                    0.7805309734513274
                ]
            ],
            "precision": [
                0.7520069882773254,
                [
                    0.7140271800012397,
                    0.7859735452093324
                ]
            ],
            "recall": [
                0.7451327433628319,
                [
                    0.7079646017699115,
                    0.7805309734513274
                ]
            ]
        },
        "nr_faulty_parsable": 1,
        "nr_non_parsable": 0
    },
    "gpt-4o-2024-08-06": {
        "metrics": {
            "f1-weighted": [
                0.7663950589473554,
                [
                    0.7316068035849045,
                    0.7993166797238859
                ]
            ],
            "accuracy": [
                0.7646017699115044,
                [
                    0.7292035398230089,
                    0.7982300884955752
                ]
            ],
            "precision": [
                0.777017067606075,
                [
                    0.7423112259072283,
                    0.8084802944866931
                ]
            ],
            "recall": [
                0.7646017699115044,
                [
                    0.7292035398230089,
                    0.7982300884955752
                ]
            ]
        },
        "nr_faulty_parsable": 0,
        "nr_non_parsable": 0
    },
    "biobert": {
        "metrics": {
            "f1-weighted": [
                0.906486212187604,
                [
                    0.8800689576185433,
                    0.9291715423395434
                ]
            ],
            "accuracy": [
                0.9061946902654868,
                [
                    0.879646017699115,
                    0.927433628318584
                ]
            ],
            "precision": [
                0.9074743305886644,
                [
                    0.8808982778187103,
                    0.928753369937492
                ]
            ],
            "recall": [
                0.9061946902654868,
                [
                    0.879646017699115,
                    0.927433628318584
                ]
            ]
        },
        "nr_faulty_parsable": 0,
        "nr_non_parsable": 0
    }
}