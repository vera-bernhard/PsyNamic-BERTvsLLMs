{
    "gpt-4o-mini_22-07-22": {
        "metrics": {
            "f1-weighted": [
                0.4562604728622199,
                [
                    0.33709854789824306,
                    0.5661976805689666
                ]
            ],
            "accuracy": [
                0.44329896907216493,
                [
                    0.35051546391752575,
                    0.5463917525773195
                ]
            ],
            "precision": [
                0.7863393639242638,
                [
                    0.5800899463834838,
                    0.8237174261391482
                ]
            ],
            "recall": [
                0.5154639175257731,
                [
                    0.41237113402061853,
                    0.6185567010309279
                ]
            ]
        },
        "nr_empty_tru": 0
    },
    "gpt-4o-mini_05-06-05": {
        "metrics": {
            "f1-weighted": [
                0.4175119681345698,
                [
                    0.3023989908606005,
                    0.5300500549359729
                ]
            ],
            "accuracy": [
                0.3917525773195876,
                [
                    0.29896907216494845,
                    0.4948453608247423
                ]
            ],
            "precision": [
                0.7739230737886049,
                [
                    0.3568085833354692,
                    0.8079404246026887
                ]
            ],
            "recall": [
                0.5051546391752577,
                [
                    0.4020618556701031,
                    0.6082474226804123
                ]
            ]
        },
        "nr_empty_tru": 0
    },
    "MeLLaMA-13B-chat_22-07-22": {
        "metrics": {
            "f1-weighted": [
                0.6906913367756741,
                [
                    0.5807281105021325,
                    0.7822267487307543
                ]
            ],
            "accuracy": [
                0.6746987951807228,
                [
                    0.5662650602409639,
                    0.7710843373493976
                ]
            ],
            "precision": [
                0.6937424115064911,
                [
                    0.5675812591974698,
                    0.7788297486043542
                ]
            ],
            "recall": [
                0.7108433734939759,
                [
                    0.6024096385542169,
                    0.7951807228915663
                ]
            ]
        },
        "nr_empty_tru": 14
    },
    "gpt-4o-2024-08-06_22-07-22": {
        "metrics": {
            "f1-weighted": [
                0.6216621967076789,
                [
                    0.5104601842619205,
                    0.7243185550540193
                ]
            ],
            "accuracy": [
                0.6391752577319587,
                [
                    0.5463917525773195,
                    0.7319587628865979
                ]
            ],
            "precision": [
                0.7892291665790384,
                [
                    0.6021856629523412,
                    0.8355817290355593
                ]
            ],
            "recall": [
                0.6391752577319587,
                [
                    0.5463917525773195,
                    0.7319587628865979
                ]
            ]
        },
        "nr_empty_tru": 0
    },
    "Llama-2-13-chat-hf_22-07-22": {
        "metrics": {
            "f1-weighted": [
                0.5284449799913717,
                [
                    0.4197783481596423,
                    0.6370921762153202
                ]
            ],
            "accuracy": [
                0.5567010309278351,
                [
                    0.4536082474226804,
                    0.6494845360824743
                ]
            ],
            "precision": [
                0.6555870246591896,
                [
                    0.5083301522099698,
                    0.7856330075738375
                ]
            ],
            "recall": [
                0.5773195876288659,
                [
                    0.4742268041237113,
                    0.6701030927835051
                ]
            ]
        },
        "nr_empty_tru": 0
    },
    "clinicalbert": {
        "metrics": {
            "f1-weighted": [
                0.8707412862052037,
                [
                    0.7905528292710534,
                    0.9268839166356126
                ]
            ],
            "accuracy": [
                0.865979381443299,
                [
                    0.7835051546391752,
                    0.9175257731958762
                ]
            ],
            "precision": [
                0.8789345720766607,
                [
                    0.7900749563969184,
                    0.9316097340295442
                ]
            ],
            "recall": [
                0.865979381443299,
                [
                    0.7835051546391752,
                    0.9175257731958762
                ]
            ]
        },
        "nr_empty_tru": 0
    }
}