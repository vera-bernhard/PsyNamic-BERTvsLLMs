{
    "Med-LLaMA3-8B": {
        "accuracy_subject": 29.896907216494846,
        "precision_subject": 2.0,
        "recall_subject": 5.612244897959184,
        "f1_subject": 2.9490616621983916,
        "accuracy_event": 29.896907216494846,
        "precision_event": 2.0,
        "recall_event": 5.612244897959184,
        "f1_event": 2.9490616621983916,
        "f1 overall - strict": 0.11764705882352941,
        "f1 overall - partial": 0.13333333333333333,
        "f1 APP - strict": 0.11250000000000002,
        "f1 APP - partial": 0.13125000000000003,
        "f1 DOS - strict": 0.1263157894736842,
        "f1 DOS - partial": 0.1368421052631579
    },
    "Llama-3.1-8B-Instruct": {
        "accuracy_subject": 0.0,
        "precision_subject": 7.4074074074074066,
        "recall_subject": 53.06122448979592,
        "f1_subject": 13.0,
        "accuracy_event": 0.0,
        "precision_event": 7.4074074074074066,
        "recall_event": 53.06122448979592,
        "f1_event": 13.0,
        "f1 overall - strict": 0.14864864864864863,
        "f1 overall - partial": 0.19401544401544402,
        "f1 APP - strict": 0.13081761006289305,
        "f1 APP - partial": 0.179874213836478,
        "f1 DOS - strict": 0.2074688796680498,
        "f1 DOS - partial": 0.2406639004149378
    },
    "tuned": {
        "accuracy_subject": 43.29896907216495,
        "precision_subject": 32.35294117647059,
        "recall_subject": 44.89795918367347,
        "f1_subject": 37.606837606837615,
        "accuracy_event": 43.29896907216495,
        "precision_event": 32.35294117647059,
        "recall_event": 44.89795918367347,
        "f1_event": 37.606837606837615,
        "f1 overall - strict": 0.42821158690176325,
        "f1 overall - partial": 0.473551637279597,
        "f1 APP - strict": 0.4105960264900662,
        "f1 APP - partial": 0.4569536423841059,
        "f1 DOS - strict": 0.4842105263157894,
        "f1 DOS - partial": 0.5263157894736842
    },
    "Llama-2-70b-chat-hf": {
        "accuracy_subject": 0.0,
        "precision_subject": 6.462035541195476,
        "recall_subject": 20.408163265306122,
        "f1_subject": 9.815950920245399,
        "accuracy_event": 0.0,
        "precision_event": 6.462035541195476,
        "recall_event": 20.408163265306122,
        "f1_event": 9.815950920245399,
        "f1 overall - strict": 0.10555555555555554,
        "f1 overall - partial": 0.17083333333333334,
        "f1 APP - strict": 0.1149425287356322,
        "f1 APP - partial": 0.22413793103448273,
        "f1 DOS - strict": 0.0967741935483871,
        "f1 DOS - partial": 0.12096774193548387
    },
    "Llama-2-13b-chat-hf": {
        "accuracy_subject": 2.0618556701030926,
        "precision_subject": 3.0201342281879198,
        "recall_subject": 9.183673469387756,
        "f1_subject": 4.545454545454546,
        "accuracy_event": 2.0618556701030926,
        "precision_event": 2.8523489932885906,
        "recall_event": 8.673469387755102,
        "f1_event": 4.292929292929292,
        "f1 overall - strict": 0.08362369337979093,
        "f1 overall - partial": 0.15156794425087108,
        "f1 APP - strict": 0.10557184750733138,
        "f1 APP - partial": 0.187683284457478,
        "f1 DOS - strict": 0.051502145922746774,
        "f1 DOS - partial": 0.09871244635193131
    },
    "MeLLaMA-70B-chat": {
        "accuracy_subject": 27.835051546391753,
        "precision_subject": 25.6198347107438,
        "recall_subject": 15.816326530612246,
        "f1_subject": 19.558359621451103,
        "accuracy_event": 27.835051546391753,
        "precision_event": 25.6198347107438,
        "recall_event": 15.816326530612246,
        "f1_event": 19.558359621451103,
        "f1 overall - strict": 0.2364217252396166,
        "f1 overall - partial": 0.28115015974440893,
        "f1 APP - strict": 0.27230046948356806,
        "f1 APP - partial": 0.30985915492957744,
        "f1 DOS - strict": 0.16,
        "f1 DOS - partial": 0.21999999999999997
    },
    "MeLLaMA-13B-chat": {
        "accuracy_subject": 12.371134020618557,
        "precision_subject": 4.590163934426229,
        "recall_subject": 14.285714285714285,
        "f1_subject": 6.9478908188585615,
        "accuracy_event": 12.371134020618557,
        "precision_event": 4.590163934426229,
        "recall_event": 14.285714285714285,
        "f1_event": 6.9478908188585615,
        "f1 overall - strict": 0.1566579634464752,
        "f1 overall - partial": 0.22454308093994776,
        "f1 APP - strict": 0.14285714285714285,
        "f1 APP - partial": 0.20982142857142858,
        "f1 DOS - strict": 0.17610062893081763,
        "f1 DOS - partial": 0.2452830188679245
    },
    "gpt-4o-2024-08-06": {
        "accuracy_subject": 15.463917525773196,
        "precision_subject": 24.346076458752517,
        "recall_subject": 62.69430051813472,
        "f1_subject": 35.07246376811594,
        "accuracy_event": 15.463917525773196,
        "precision_event": 24.14486921529175,
        "recall_event": 62.17616580310881,
        "f1_event": 34.78260869565217,
        "f1 overall - strict": 0.28943560057887124,
        "f1 overall - partial": 0.3820549927641099,
        "f1 APP - strict": 0.21928166351606806,
        "f1 APP - partial": 0.3289224952741021,
        "f1 DOS - strict": 0.5185185185185185,
        "f1 DOS - partial": 0.5555555555555556
    },
    "Meta-Llama-3-8B-Instruct": {
        "accuracy_subject": 0.0,
        "precision_subject": 15.669014084507044,
        "recall_subject": 45.40816326530612,
        "f1_subject": 23.298429319371728,
        "accuracy_event": 0.0,
        "precision_event": 15.492957746478872,
        "recall_event": 44.89795918367347,
        "f1_event": 23.03664921465969,
        "f1 overall - strict": 0.21353383458646616,
        "f1 overall - partial": 0.2857142857142857,
        "f1 APP - strict": 0.20657276995305163,
        "f1 APP - partial": 0.2934272300469484,
        "f1 DOS - strict": 0.22594142259414232,
        "f1 DOS - partial": 0.2719665271966527
    },
    "gpt-4o-mini": {
        "accuracy_subject": 0.0,
        "precision_subject": 8.841201716738198,
        "recall_subject": 53.36787564766839,
        "f1_subject": 15.169366715758468,
        "accuracy_event": 0.0,
        "precision_event": 8.755364806866952,
        "recall_event": 52.84974093264248,
        "f1_event": 15.022091310751106,
        "f1 overall - strict": 0.12796208530805686,
        "f1 overall - partial": 0.19510268562401267,
        "f1 APP - strict": 0.12351029252437704,
        "f1 APP - partial": 0.1917659804983749,
        "f1 DOS - strict": 0.13994169096209913,
        "f1 DOS - partial": 0.20408163265306126
    }
}